{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNPNF5TcxU4f9gFJ3wdX0gf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frans-nekongo/docs/blob/main/RAG_OldMutual_gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2db91ddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8e394d-9821-4c6e-82b5-818eb68bdf5d"
      },
      "source": [
        "# Install system dependencies for OCR\n",
        "!apt-get update && apt-get install -y poppler-utils tesseract-ocr\n",
        "\n",
        "# Install Python libraries\n",
        "# pytesseract, pillow, pdf2image: for OCR\n",
        "# pypdf: for PDF text extraction\n",
        "# google-generativeai: for generating embeddings with Gemini\n",
        "# supabase: for interacting with your Supabase database\n",
        "# tqdm: for displaying progress bars\n",
        "# langchain-text-splitters: dedicated package for text splitting\n",
        "!pip install -q pytesseract pillow pdf2image pypdf tqdm \\\n",
        "               google-generativeai supabase==2.6.0 \\\n",
        "               langchain-community langchain langchain-text-splitters\n",
        "\n",
        "# === Part 0: Mount Google Drive and Define Data Directory ===\n",
        "print(\"Mounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted.\")\n",
        "\n",
        "# Define your base data directory on Google Drive\n",
        "DATA_ROOT_DIR = \"/content/drive/MyDrive/RAGBook\"\n",
        "print(f\"Using data directory: {DATA_ROOT_DIR}\")\n",
        "\n",
        "import os\n",
        "if not os.path.exists(DATA_ROOT_DIR):\n",
        "    print(f\"Warning: The specified data directory does not exist: {DATA_ROOT_DIR}. Please create it.\")\n",
        "else:\n",
        "    print(f\"Contents of {DATA_ROOT_DIR}:\")\n",
        "    for item in os.listdir(DATA_ROOT_DIR):\n",
        "        print(f\"- {item}\")\n",
        "\n",
        "# === Part 1: Imports ===\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "from supabase import create_client\n",
        "# Updated import to use the dedicated package\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Part 3: Load and Process the Book (with OCR integration) ===\n",
        "\n",
        "def extract_text_from_pdf_with_ocr(pdf_path, dpi=300):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF, including text from images using OCR.\n",
        "    Returns: A list of LangChain Document objects.\n",
        "    \"\"\"\n",
        "    print(f\"Extracting text from {pdf_path} with OCR...\")\n",
        "    documents = []\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path, dpi=dpi)\n",
        "        print(f\"Converted {len(images)} pages to images.\")\n",
        "\n",
        "        for i, img in enumerate(images):\n",
        "            print(f\"Processing page {i+1}...\")\n",
        "            try:\n",
        "                text = pytesseract.image_to_string(img)\n",
        "                doc = Document(page_content=text, metadata={\"source\": pdf_path, \"page\": i})\n",
        "                documents.append(doc)\n",
        "            except Exception as ocr_error:\n",
        "                print(f\"Error processing page {i+1} with OCR: {ocr_error}\")\n",
        "                doc = Document(page_content=\"\", metadata={\"source\": pdf_path, \"page\": i})\n",
        "                documents.append(doc)\n",
        "\n",
        "        print(\"Text extraction complete.\")\n",
        "        return documents\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting PDF to images or during OCR: {e}\")\n",
        "        return []\n",
        "\n",
        "# Specify the name of the PDF file\n",
        "pdf_filename = 'OMP.pdf'\n",
        "pdf_path = os.path.join(DATA_ROOT_DIR, pdf_filename)\n",
        "\n",
        "documents = []\n",
        "if os.path.exists(pdf_path):\n",
        "    documents = extract_text_from_pdf_with_ocr(pdf_path)\n",
        "    if not documents:\n",
        "        print(f\"Could not extract text from {pdf_path}. Please check the file and try again.\")\n",
        "    else:\n",
        "        print(f\"Successfully extracted text from {len(documents)} pages.\")\n",
        "else:\n",
        "    print(f\"PDF file not found at {pdf_path}\")\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print(f\"Created {len(splits)} text chunks after splitting.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [1 InRelease 14.2 kB/129 k\r                                                                               \rGet:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [1 InRelease 14.2 kB/129 k\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [1 InRelease 31.5 kB/129 k\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [1 InRelease 31.5 kB/129 k\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connecting to cloud.r-pro\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connecting to cloud.r-pro\r                                                                               \rHit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Connecting to r2u\r                                                                               \rHit:5 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Connected to r2u.\r                                                                               \rHit:6 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Connected to r2u.\r0% [Waiting for headers] [Connecting to cloud.r-project.org (65.9.86.109)] [Con\r                                                                               \rHit:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:11 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Fetched 133 kB in 1s (143 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 46 not upgraded.\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.52.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "google-genai 1.52.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 12.0 which is incompatible.\n",
            "google-adk 1.17.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 12.0 which is incompatible.\n",
            "yfinance 0.2.66 requires websockets>=13.0, but you have websockets 12.0 which is incompatible.\n",
            "gradio-client 1.13.3 requires websockets<16.0,>=13.0, but you have websockets 12.0 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires websockets>=14.0, but you have websockets 12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mMounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted.\n",
            "Using data directory: /content/drive/MyDrive/RAGBook\n",
            "Contents of /content/drive/MyDrive/RAGBook:\n",
            "- learners_book.pdf\n",
            "- .ipynb_checkpoints\n",
            "- learnersbookmin.pdf\n",
            "- OMP.pdf\n",
            "Extracting text from /content/drive/MyDrive/RAGBook/OMP.pdf with OCR...\n",
            "Converted 49 pages to images.\n",
            "Processing page 1...\n",
            "Processing page 2...\n",
            "Processing page 3...\n",
            "Processing page 4...\n",
            "Processing page 5...\n",
            "Processing page 6...\n",
            "Processing page 7...\n",
            "Processing page 8...\n",
            "Processing page 9...\n",
            "Processing page 10...\n",
            "Processing page 11...\n",
            "Processing page 12...\n",
            "Processing page 13...\n",
            "Processing page 14...\n",
            "Processing page 15...\n",
            "Processing page 16...\n",
            "Processing page 17...\n",
            "Processing page 18...\n",
            "Processing page 19...\n",
            "Processing page 20...\n",
            "Processing page 21...\n",
            "Processing page 22...\n",
            "Processing page 23...\n",
            "Processing page 24...\n",
            "Processing page 25...\n",
            "Processing page 26...\n",
            "Processing page 27...\n",
            "Processing page 28...\n",
            "Processing page 29...\n",
            "Processing page 30...\n",
            "Processing page 31...\n",
            "Processing page 32...\n",
            "Processing page 33...\n",
            "Processing page 34...\n",
            "Processing page 35...\n",
            "Processing page 36...\n",
            "Processing page 37...\n",
            "Processing page 38...\n",
            "Processing page 39...\n",
            "Processing page 40...\n",
            "Processing page 41...\n",
            "Processing page 42...\n",
            "Processing page 43...\n",
            "Processing page 44...\n",
            "Processing page 45...\n",
            "Processing page 46...\n",
            "Processing page 47...\n",
            "Processing page 48...\n",
            "Processing page 49...\n",
            "Text extraction complete.\n",
            "Successfully extracted text from 49 pages.\n",
            "Created 178 text chunks after splitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e58c02c5",
        "outputId": "ccaed842-28e5-43a2-c750-ac4abffe94dd"
      },
      "source": [
        "# Install the new Google GenAI SDK and upgrade Supabase to resolve dependency conflicts\n",
        "!pip install -U google-genai supabase"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.52.0)\n",
            "Requirement already satisfied: supabase in /usr/local/lib/python3.12/dist-packages (2.6.0)\n",
            "Collecting supabase\n",
            "  Using cached supabase-2.24.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.38.0)\n",
            "Collecting httpx<1.0.0,>=0.28.1 (from google-genai)\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.11.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.32.5)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (8.5.0)\n",
            "Collecting websockets<15.1.0,>=13.0.0 (from google-genai)\n",
            "  Using cached websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Collecting realtime==2.24.0 (from supabase)\n",
            "  Using cached realtime-2.24.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: supabase-functions==2.24.0 in /usr/local/lib/python3.12/dist-packages (from supabase) (2.24.0)\n",
            "Collecting storage3==2.24.0 (from supabase)\n",
            "  Using cached storage3-2.24.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: supabase-auth==2.24.0 in /usr/local/lib/python3.12/dist-packages (from supabase) (2.24.0)\n",
            "Collecting postgrest==2.24.0 (from supabase)\n",
            "  Using cached postgrest-2.24.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: deprecation>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from postgrest==2.24.0->supabase) (2.1.0)\n",
            "Requirement already satisfied: yarl>=1.20.1 in /usr/local/lib/python3.12/dist-packages (from postgrest==2.24.0->supabase) (1.22.0)\n",
            "Requirement already satisfied: pyjwt>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->supabase-auth==2.24.0->supabase) (2.10.1)\n",
            "Requirement already satisfied: strenum>=0.4.15 in /usr/local/lib/python3.12/dist-packages (from supabase-functions==2.24.0->supabase) (0.4.15)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation>=2.1.0->postgrest==2.24.0->supabase) (25.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]<0.29,>=0.26->postgrest==2.24.0->supabase) (4.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->supabase-auth==2.24.0->supabase) (43.0.3)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.12/dist-packages (from yarl>=1.20.1->postgrest==2.24.0->supabase) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from yarl>=1.20.1->postgrest==2.24.0->supabase) (0.4.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->supabase-auth==2.24.0->supabase) (2.0.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest==2.24.0->supabase) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest==2.24.0->supabase) (4.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->supabase-auth==2.24.0->supabase) (2.23)\n",
            "Using cached supabase-2.24.0-py3-none-any.whl (16 kB)\n",
            "Using cached postgrest-2.24.0-py3-none-any.whl (21 kB)\n",
            "Using cached realtime-2.24.0-py3-none-any.whl (22 kB)\n",
            "Using cached storage3-2.24.0-py3-none-any.whl (19 kB)\n",
            "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Using cached websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
            "Installing collected packages: websockets, httpx, realtime, storage3, postgrest, supabase\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 12.0\n",
            "    Uninstalling websockets-12.0:\n",
            "      Successfully uninstalled websockets-12.0\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.27.2\n",
            "    Uninstalling httpx-0.27.2:\n",
            "      Successfully uninstalled httpx-0.27.2\n",
            "  Attempting uninstall: realtime\n",
            "    Found existing installation: realtime 1.0.6\n",
            "    Uninstalling realtime-1.0.6:\n",
            "      Successfully uninstalled realtime-1.0.6\n",
            "  Attempting uninstall: storage3\n",
            "    Found existing installation: storage3 0.7.7\n",
            "    Uninstalling storage3-0.7.7:\n",
            "      Successfully uninstalled storage3-0.7.7\n",
            "  Attempting uninstall: postgrest\n",
            "    Found existing installation: postgrest 0.16.11\n",
            "    Uninstalling postgrest-0.16.11:\n",
            "      Successfully uninstalled postgrest-0.16.11\n",
            "  Attempting uninstall: supabase\n",
            "    Found existing installation: supabase 2.6.0\n",
            "    Uninstalling supabase-2.6.0:\n",
            "      Successfully uninstalled supabase-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "supafunc 0.5.1 requires httpx[http2]<0.28,>=0.24, but you have httpx 0.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed httpx-0.28.1 postgrest-2.24.0 realtime-2.24.0 storage3-2.24.0 supabase-2.24.0 websockets-15.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "httpx",
                  "postgrest",
                  "realtime",
                  "storage3",
                  "supabase",
                  "websockets"
                ]
              },
              "id": "df82aed36bc644818ccec9cec93cf017"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Part 4: Configure Google Gemini and Supabase for Embeddings and Upload ===\n",
        "\n",
        "from google.colab import userdata\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "# Import the new SDK\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from supabase import create_client\n",
        "\n",
        "# Retrieve API keys and URL from Colab Secrets\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "SUPABASE_URL_SECRET = userdata.get('SUPABASE_URL')\n",
        "SUPABASE_SERVICE_ROLE_KEY_SECRET = userdata.get('SUPABASE_SERVICE_ROLE_KEY')\n",
        "\n",
        "# Validate secrets\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"GOOGLE_API_KEY not found in Colab Secrets. Please add it.\")\n",
        "if not SUPABASE_URL_SECRET:\n",
        "    raise ValueError(\"SUPABASE_URL not found in Colab Secrets. Please add it.\")\n",
        "if not SUPABASE_SERVICE_ROLE_KEY_SECRET:\n",
        "    raise ValueError(\"SUPABASE_SERVICE_ROLE_KEY not found in Colab Secrets. Please add it.\")\n",
        "\n",
        "# Configure the new GenAI Client\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Configure Supabase\n",
        "supabase = create_client(SUPABASE_URL_SECRET, SUPABASE_SERVICE_ROLE_KEY_SECRET)\n",
        "\n",
        "# Configure embedding model and dimensions\n",
        "EMBED_MODEL = \"gemini-embedding-001\"\n",
        "EMBED_DIMS = 3072\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "# Target Table Name\n",
        "TABLE_NAME = \"oldMutualDocs\"\n",
        "\n",
        "# Prepare records for insertion from LangChain Document 'splits'\n",
        "print(\"Preparing records for insertion into Supabase...\")\n",
        "records = []\n",
        "# Ensure 'splits' is available from previous cells\n",
        "if 'splits' not in locals():\n",
        "    print(\"Error: 'splits' variable not found. Please run the previous cells to process the PDF.\")\n",
        "    records = []\n",
        "else:\n",
        "    for d in splits:\n",
        "        content = d.page_content\n",
        "        meta = d.metadata or {}\n",
        "        if \"page\" in meta and isinstance(meta[\"page\"], int):\n",
        "            meta[\"page\"] = int(meta[\"page\"]) + 1\n",
        "        else:\n",
        "            meta[\"page\"] = \"N/A\"\n",
        "\n",
        "        records.append({\n",
        "            \"doc_id\": pdf_filename if 'pdf_filename' in locals() else \"unknown.pdf\",\n",
        "            \"content\": content,\n",
        "            \"metadata\": meta\n",
        "        })\n",
        "\n",
        "# Batch-embed with Gemini and insert into Supabase\n",
        "print(f\"Processing {len(records)} records in batches of {BATCH_SIZE} for embedding and upload to '{TABLE_NAME}'...\")\n",
        "inserted_count = 0\n",
        "\n",
        "for i in tqdm(range(0, len(records), BATCH_SIZE), desc=\"Uploading to Supabase\"):\n",
        "    batch_records = records[i:i+BATCH_SIZE]\n",
        "    texts_to_embed = [r[\"content\"] for r in batch_records]\n",
        "\n",
        "    if not texts_to_embed:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Generate embeddings using the new Client SDK\n",
        "        response = client.models.embed_content(\n",
        "            model=EMBED_MODEL,\n",
        "            contents=texts_to_embed,\n",
        "            config=types.EmbedContentConfig(\n",
        "                output_dimensionality=EMBED_DIMS,\n",
        "                task_type=\"RETRIEVAL_DOCUMENT\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Extract embeddings from the response object\n",
        "        # The new SDK returns a list of embedding objects\n",
        "        embeddings_list = [e.values for e in response.embeddings]\n",
        "\n",
        "        # Validation check for the first batch\n",
        "        if i == 0 and embeddings_list and len(embeddings_list[0]) != EMBED_DIMS:\n",
        "             print(f\"Warning: Model returned {len(embeddings_list[0])} dimensions, expected {EMBED_DIMS}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError getting embeddings for batch {i//BATCH_SIZE}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Prepare rows for Supabase insertion\n",
        "    rows_to_insert = []\n",
        "    for r_data, emb_vec in zip(batch_records, embeddings_list):\n",
        "        rows_to_insert.append({\n",
        "            \"doc_id\": r_data[\"doc_id\"],\n",
        "            \"content\": r_data[\"content\"],\n",
        "            \"embedding\": emb_vec,\n",
        "            \"metadata\": json.dumps(r_data[\"metadata\"])\n",
        "        })\n",
        "\n",
        "    # Insert batch into Supabase\n",
        "    if rows_to_insert:\n",
        "        try:\n",
        "            response = supabase.table(TABLE_NAME).insert(rows_to_insert).execute()\n",
        "            if hasattr(response, 'error') and response.error:\n",
        "                print(f\"\\nSupabase insert error for batch {i//BATCH_SIZE}: {response.error}\")\n",
        "            else:\n",
        "                inserted_count += len(rows_to_insert)\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError inserting batch {i//BATCH_SIZE} into Supabase: {e}\")\n",
        "\n",
        "print(f\"\\nData upload complete. Successfully inserted {inserted_count} records into Supabase table '{TABLE_NAME}'.\")"
      ],
      "metadata": {
        "id": "f8bgEidjJZDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf879f22-59c4-4860-9a98-2385b2467599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing records for insertion into Supabase...\n",
            "Processing 178 records in batches of 100 for embedding and upload to 'oldMutualDocs'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Uploading to Supabase: 100%|██████████| 2/2 [00:05<00:00,  2.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data upload complete. Successfully inserted 178 records into Supabase table 'oldMutualDocs'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}